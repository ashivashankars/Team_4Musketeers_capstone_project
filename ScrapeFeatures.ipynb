{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "567aaf97",
        "d4662400",
        "9e1ca71d",
        "4ac7d207",
        "69acb004"
      ],
      "authorship_tag": "ABX9TyMg12cHdhI+4qeHVvJP1Za/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/Team_4Musketeers_capstone_project/blob/main/ScrapeFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c61f41c"
      },
      "source": [
        "# Task\n",
        "Apply NLP techniques to extract 'Years of Experience', 'Diploma Type', 'Job Type', 'Sponsorship Requirement', and 'Skill Sets' from the text content in the `/content/all_internships_with_content.csv` file, and then save the augmented data to a new CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff8a75ac"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the `/content/all_internships_with_content.csv` file into a pandas DataFrame and inspect its structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66948471"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the CSV file into a pandas DataFrame, display the first 5 rows, and print a concise summary to inspect its structure, as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX1KYr9TZVuE",
        "outputId": "07be96f9-2d7f-4978-de02-81a9617f7370"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40daa935",
        "outputId": "6a4f1c1a-d9b4-4e7e-ad5a-edc1c0ff578a"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "# Get the current field size limit\n",
        "current_limit = csv.field_size_limit()\n",
        "\n",
        "# Set a new, larger field size limit to handle large fields\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "try:\n",
        "    # Try reading the CSV with 'on_bad_lines='skip' to bypass problematic rows\n",
        "    # Use header=None to prevent the first row from being interpreted as headers\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/all_internships_with_content.csv', engine='python', on_bad_lines='skip', header=None)\n",
        "\n",
        "    # Set the first row as the header and then drop the first row\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df[1:].reset_index(drop=True)\n",
        "\n",
        "    print(\"First 20 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nConcise summary of the DataFrame:\")\n",
        "    df.info()\n",
        "finally:\n",
        "    # Reset the field size limit to its original value\n",
        "    csv.field_size_limit(current_limit)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 rows of the DataFrame:\n",
            "0                  Company                               Role  \\\n",
            "0  The Walt Disney Company                Data Product Intern   \n",
            "1                Teamworks          Product Management Intern   \n",
            "2                  Zscaler     Product Management Intern - AI   \n",
            "3              üî•ServiceNow         Portfolio Associate Intern   \n",
            "4                Santander  Digital Product Management Intern   \n",
            "\n",
            "0         Location                                        Application Age  \\\n",
            "0  Celebration, FL  https://disney.wd5.myworkdayjobs.com/disneycar...  0d   \n",
            "1    Remote in USA  https://ats.rippling.com/teamworks-careers/job...  0d   \n",
            "2     San Jose, CA  https://job-boards.greenhouse.io/zscaler/jobs/...  1d   \n",
            "3      Chicago, IL  https://jobs.smartrecruiters.com/ServiceNow/74...  1d   \n",
            "4       Boston, MA  https://santander.wd3.myworkdayjobs.com/Santan...  1d   \n",
            "\n",
            "0                                           raw_html  \\\n",
            "0  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head>\\n...   \n",
            "1  <!DOCTYPE html><html lang=\"en-US\"><head><meta ...   \n",
            "2  <!DOCTYPE html><html lang=\"en\" dir=\"ltr\"><head...   \n",
            "3  <!DOCTYPE html><html lang=\"en\"><head><!-- spl-...   \n",
            "4  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head>\\n...   \n",
            "\n",
            "0                                       text_content  \n",
            "0                                                NaN  \n",
            "1  Product Management MBA Intern (Remote) Product...  \n",
            "2  Job Application for Product Management Intern,...  \n",
            "3  ServiceNow Portfolio Associate Intern | SmartR...  \n",
            "4                                                NaN  \n",
            "\n",
            "Concise summary of the DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1582 entries, 0 to 1581\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Company       1582 non-null   object\n",
            " 1   Role          1582 non-null   object\n",
            " 2   Location      1582 non-null   object\n",
            " 3   Application   1582 non-null   object\n",
            " 4   Age           1582 non-null   object\n",
            " 5   raw_html      1524 non-null   object\n",
            " 6   text_content  981 non-null    object\n",
            "dtypes: object(7)\n",
            "memory usage: 86.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567aaf97"
      },
      "source": [
        "## Extract Years of Experience (YOE)\n",
        "\n",
        "### Subtask:\n",
        "Develop and apply NLP techniques (e.g., regular expressions) to extract numeric values representing 'Years of Experience' from the text content. The extracted value will be converted to a numeric type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4662400"
      },
      "source": [
        "### Subtask\n",
        "Develop and apply NLP techniques (e.g., regular expressions) to extract numeric values representing 'Years of Experience' from the text content. The extracted value will be converted to a numeric type.\n",
        "\n",
        "#### Instructions\n",
        "1. Fill any missing values in the 'text_content' column with an empty string to ensure regex operations do not fail.\n",
        "2. Define a regular expression pattern to capture numerical values related to 'years of experience'. Consider variations like 'X years', 'X+ years', 'X-Y years', or 'minimum X years'.\n",
        "3. Apply the defined regex pattern to the 'text_content' column of the DataFrame `df` to extract potential years of experience.\n",
        "4. From the extracted matches, determine the most appropriate single numeric value (e.g., the lower bound for ranges like '3-5 years', or the number itself for '3+ years').\n",
        "5. Convert the extracted numeric values to an integer or float type.\n",
        "6. Store the extracted and converted 'Years of Experience' in a new column named 'YOE' in the DataFrame `df`. If no experience is found, assign a default value like 0 or NaN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc995bc9"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to fill any missing values in the 'text_content' column with an empty string to ensure that subsequent regex operations do not encounter errors. This is a crucial data preparation step before text processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242eae8e",
        "outputId": "fb9ad170-6a7f-4f8d-bbfb-7870d8db0607"
      },
      "source": [
        "df['text_content'] = df['text_content'].fillna('')\n",
        "print(\"Missing values in 'text_content' after filling NaNs:\")\n",
        "print(df['text_content'].isnull().sum())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'text_content' after filling NaNs:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f181b6f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, the next step is to define a regular expression pattern to capture various formats of 'years of experience' and apply it to the 'text_content' column. This will involve using `re.findall` or `str.findall` to get all potential matches for each entry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b395cf9d",
        "outputId": "b96ac4fd-8947-409e-e454-8b468464f57e"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define a regex pattern to capture years of experience\n",
        "# This pattern looks for numbers (X, X+, X-Y) followed by 'year(s)' with optional words in between.\n",
        "# It also considers 'minimum X years' and similar phrases.\n",
        "experience_pattern = re.compile(r'(\\d+\\+?|-?\\d*\\.?\\d+\\s*(?:to|-)\\s*\\d*\\.?\\d+|\\d+)\\s*(?:years?|yrs?|yr)\\s*(?:of\\s*experience|experience)?|minimum\\s*(\\d+)\\s*(?:years?|yrs?|yr)')\n",
        "\n",
        "# Apply the pattern to the 'text_content' column to find all matches\n",
        "df['extracted_yoe_matches'] = df['text_content'].apply(lambda x: experience_pattern.findall(x.lower()))\n",
        "\n",
        "print(\"First 5 rows of 'extracted_yoe_matches' column:\")\n",
        "print(df['extracted_yoe_matches'].head())\n",
        "\n",
        "print(\"\\nNumber of rows with extracted experience matches:\")\n",
        "print(df[df['extracted_yoe_matches'].apply(lambda x: len(x) > 0)].shape[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of 'extracted_yoe_matches' column:\n",
            "0          []\n",
            "1    [(3+, )]\n",
            "2          []\n",
            "3          []\n",
            "4          []\n",
            "Name: extracted_yoe_matches, dtype: object\n",
            "\n",
            "Number of rows with extracted experience matches:\n",
            "149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a25d5cf"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to process the `extracted_yoe_matches` column to derive a single numeric value for 'Years of Experience' (YOE) for each entry, handling different formats like ranges or 'X+'. This involves iterating through the extracted matches, parsing them, and converting them to a numeric type, storing the result in the 'YOE' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2edb18e3",
        "outputId": "5bace02e-4e89-4ef5-fd04-a5c0cdb52599"
      },
      "source": [
        "def parse_yoe(matches):\n",
        "    yoe_values = []\n",
        "    for match_tuple in matches:\n",
        "        # The regex has two main capturing groups: one for standard patterns and one for 'minimum X years'\n",
        "        # We need to check both parts of the tuple\n",
        "        for match_str in match_tuple:\n",
        "            if match_str:\n",
        "                # Handle ranges like '3-5 years' or '3 to 5 years'\n",
        "                if 'to' in match_str or '-' in match_str:\n",
        "                    parts = re.findall(r'\\d+', match_str)\n",
        "                    if parts: # Take the lower bound of the range\n",
        "                        yoe_values.append(int(parts[0]))\n",
        "                # Handle 'X+ years'\n",
        "                elif '+' in match_str:\n",
        "                    yoe_values.append(int(match_str.replace('+', '')))\n",
        "                # Handle simple 'X years' or 'minimum X years'\n",
        "                else:\n",
        "                    try:\n",
        "                        yoe_values.append(int(match_str))\n",
        "                    except ValueError:\n",
        "                        pass # Ignore non-numeric strings that might slip through\n",
        "\n",
        "    if yoe_values:\n",
        "        # Return the minimum years of experience found, as typically a requirement is 'at least X years'\n",
        "        return min(yoe_values)\n",
        "    return None # Return None if no valid YOE found\n",
        "\n",
        "df['YOE'] = df['extracted_yoe_matches'].apply(parse_yoe)\n",
        "\n",
        "# Fill any remaining None values with a default (e.g., 0 or NaN)\n",
        "# Using NaN to distinguish between 'no experience found' and '0 years experience'\n",
        "df['YOE'] = df['YOE'].fillna(0) # Default to 0 if nothing is found, as per instruction to assign 0 or NaN\n",
        "\n",
        "print(\"First 5 rows of 'YOE' column:\")\n",
        "print(df['YOE'].head())\n",
        "\n",
        "print(\"\\nValue counts for 'YOE' column (showing non-zero values):\")\n",
        "print(df[df['YOE'] > 0]['YOE'].value_counts().sort_index())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of 'YOE' column:\n",
            "0    0.0\n",
            "1    3.0\n",
            "2    0.0\n",
            "3    0.0\n",
            "4    0.0\n",
            "Name: YOE, dtype: float64\n",
            "\n",
            "Value counts for 'YOE' column (showing non-zero values):\n",
            "YOE\n",
            "1.0      35\n",
            "2.0      20\n",
            "3.0      12\n",
            "5.0       1\n",
            "6.0       4\n",
            "8.0       1\n",
            "10.0      3\n",
            "15.0      1\n",
            "16.0      1\n",
            "18.0     25\n",
            "25.0      1\n",
            "28.0      2\n",
            "30.0      2\n",
            "35.0      1\n",
            "40.0      6\n",
            "50.0      1\n",
            "65.0      1\n",
            "70.0      1\n",
            "130.0     1\n",
            "175.0     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1ca71d"
      },
      "source": [
        "## Extract Diploma Type\n",
        "\n",
        "### Subtask:\n",
        "Implement NLP logic to identify and categorize educational diplomas (e.g., 'None', 'Bachelor', 'Master', 'PhD') from the text. The extraction will map to the specified enum values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bc8f585"
      },
      "source": [
        "#### Instructions Breakdown for Diploma Type Extraction\n",
        "\n",
        "To effectively extract and categorize diploma types, I will follow these steps:\n",
        "\n",
        "1.  **Define Diploma Types and Priority**: A dictionary will be created to store standardized diploma names (e.g., 'PhD', 'Master', 'Bachelor', 'Associate', 'High School', 'None') and their corresponding regular expression patterns. The order of keys in this dictionary will reflect the priority for extraction, from highest to lowest educational attainment.\n",
        "\n",
        "2.  **Regular Expression Patterns**: For each diploma type, I will craft robust regular expression patterns that account for common abbreviations, full names, and variations (e.g., 'PhD', 'doctorate', 'master's degree', 'MS', 'bachelor', 'BS', 'high school diploma'). These patterns will be case-insensitive.\n",
        "\n",
        "3.  **Iterative Application**: A function will be defined to iterate through the text content. It will attempt to match diploma patterns in the predefined priority order. The first match found will determine the 'Diploma Type' for that entry.\n",
        "\n",
        "4.  **Assign to 'Diploma' Column**: The extracted diploma type will be stored in a new column named 'Diploma' in the DataFrame `df`.\n",
        "\n",
        "5.  **Handle Missing Values**: If no diploma type is identified after checking all patterns, 'None' will be assigned to the 'Diploma' column to indicate no specific educational qualification was found or required.\n",
        "\n",
        "6.  **Verify Extraction**: Finally, the value counts of the newly created 'Diploma' column will be printed to provide an overview of the distribution of extracted educational qualifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "993e6861"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, following the outlined plan, I will define the prioritized diploma types and their corresponding regular expression patterns in a dictionary. This will serve as the core logic for identifying educational qualifications within the text content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55e2275b",
        "outputId": "a6f6be0a-0455-46e2-e09f-1168f288f1d7"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define diploma types and their regex patterns in order of priority (highest to lowest education)\n",
        "diploma_patterns = {\n",
        "    'PhD': [\n",
        "        r'ph\\.d\\b',\n",
        "        r'doctorate\\b',\n",
        "        r'doctoral\\s*degree\\b',\n",
        "        r'doctor\\s*of\\s*philosophy\\b'\n",
        "    ],\n",
        "    'Master': [\n",
        "        r'master\\.?\\s*degree\\b',\n",
        "        r'm\\.s\\b',\n",
        "        r'm\\.a\\b',\n",
        "        r'mba\\b',\n",
        "        r'msc\\b',\n",
        "        r'graduate\\s*degree\\b'\n",
        "    ],\n",
        "    'Bachelor': [\n",
        "        r'bachelor\\.?\\s*degree\\b',\n",
        "        r'b\\.s\\b',\n",
        "        r'b\\.a\\b',\n",
        "        r'bsc\\b',\n",
        "        r'undergraduate\\s*degree\\b'\n",
        "    ],\n",
        "    'Associate': [\n",
        "        r'associate\\.?\\s*degree\\b',\n",
        "        r'a\\.a\\b',\n",
        "        r'a\\.s\\b'\n",
        "    ],\n",
        "    'High School': [\n",
        "        r'high\\s*school\\s*diploma\\b',\n",
        "        r'high\\s*school\\s*graduate\\b',\n",
        "        r'ged\\b'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Diploma patterns defined successfully.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diploma patterns defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cec6c03a"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the defined `diploma_patterns`, I will now create a function to apply these patterns to the `text_content` column. This function will iterate through the patterns in order of priority, assign the first found diploma type to a new 'Diploma' column, and default to 'None' if no match is found, then print the value counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d065f1b",
        "outputId": "71015347-3278-4f5a-d41a-8d5e304b3c0f"
      },
      "source": [
        "def extract_diploma(text):\n",
        "    text = str(text).lower() # Ensure text is string and lowercased for case-insensitive matching\n",
        "    for diploma_type, patterns in diploma_patterns.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text):\n",
        "                return diploma_type\n",
        "    return 'None'\n",
        "\n",
        "df['Diploma'] = df['text_content'].apply(extract_diploma)\n",
        "\n",
        "print(\"Value counts for 'Diploma' column:\")\n",
        "print(df['Diploma'].value_counts())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for 'Diploma' column:\n",
            "Diploma\n",
            "None           1294\n",
            "High School     198\n",
            "Master           51\n",
            "PhD              31\n",
            "Bachelor          8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac7d207"
      },
      "source": [
        "## Extract Job Type\n",
        "\n",
        "### Subtask:\n",
        "Use NLP to determine the job type (e.g., 'Intern', 'Full-Time', 'Both') from relevant text fields in the dataset. This will involve keyword matching and contextual analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a385443c"
      },
      "source": [
        "#### Reasoning:\n",
        "To begin extracting 'Job Type', the first step is to define a comprehensive set of regular expression patterns for each target job type: 'Intern', 'Full-Time', 'Part-Time', and 'Contract'. These patterns will account for variations in wording and abbreviations. This setup is crucial for the subsequent NLP logic, allowing for accurate and prioritized identification of job types within the text content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cb23c18"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will define the `job_type_patterns` dictionary with regex patterns for 'Intern', 'Full-Time', 'Part-Time', and 'Contract' job types, prioritizing 'Intern' and 'Full-Time' for more common scenarios as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84c46103",
        "outputId": "b6274b83-e8fa-4911-9343-8427d2847b3c"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define job types and their regex patterns in order of priority\n",
        "job_type_patterns = {\n",
        "    'Intern': [\n",
        "        r'internship\\b',\n",
        "        r'intern\\b',\n",
        "        r'co-op\\b',\n",
        "        r'coop\\b'\n",
        "    ],\n",
        "    'Full-Time': [\n",
        "        r'full-time\\b',\n",
        "        r'full\\s*time\\b',\n",
        "        r'f/t\\b'\n",
        "    ],\n",
        "    'Part-Time': [\n",
        "        r'part-time\\b',\n",
        "        r'part\\s*time\\b',\n",
        "        r'p/t\\b'\n",
        "    ],\n",
        "    'Contract': [\n",
        "        r'contract\\b',\n",
        "        r'freelance\\b'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Job type patterns defined successfully.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job type patterns defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac3a8a8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the job type patterns are defined, I will create a function to iterate through the `text_content` column, apply these patterns in priority order, and assign the first matching job type to a new 'Job_type' column. If no match is found, it will default to 'None'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46c5190f",
        "outputId": "8226733a-66da-4270-9495-4049807375b1"
      },
      "source": [
        "def extract_job_type(text):\n",
        "    text = str(text).lower() # Ensure text is string and lowercased for case-insensitive matching\n",
        "    found_job_types = []\n",
        "\n",
        "    # Check for 'Intern' first due to its common requirement for internships\n",
        "    for pattern in job_type_patterns['Intern']:\n",
        "        if re.search(pattern, text):\n",
        "            found_job_types.append('Intern')\n",
        "            # If 'Intern' is found, it's typically the primary job type for this dataset\n",
        "            return 'Intern'\n",
        "\n",
        "    # Check for other job types if 'Intern' is not found\n",
        "    for job_type, patterns in job_type_patterns.items():\n",
        "        if job_type == 'Intern': # Skip intern as it's already checked and returned if found\n",
        "            continue\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text):\n",
        "                found_job_types.append(job_type)\n",
        "\n",
        "    if len(found_job_types) > 1:\n",
        "        # If multiple non-intern job types are found, and no intern was found,\n",
        "        # we need to decide what to return. For simplicity, we can prioritize or return 'Both'.\n",
        "        # As the instruction is 'Both' or the most appropriate primary, let's return 'Both'\n",
        "        # if more than one distinct type is detected (excluding 'Intern' as it's handled).\n",
        "        # A more sophisticated logic might be needed here based on domain knowledge.\n",
        "        # For this subtask, let's consider the first one found or 'Both' if ambiguous.\n",
        "        if 'Full-Time' in found_job_types and 'Part-Time' in found_job_types:\n",
        "            return 'Both'\n",
        "        elif 'Full-Time' in found_job_types: # Prioritize Full-Time over others if present\n",
        "            return 'Full-Time'\n",
        "        elif 'Part-Time' in found_job_types:\n",
        "            return 'Part-Time'\n",
        "        elif 'Contract' in found_job_types:\n",
        "            return 'Contract'\n",
        "    elif len(found_job_types) == 1:\n",
        "        return found_job_types[0]\n",
        "    return 'None'\n",
        "\n",
        "# Apply the function to the 'text_content' column\n",
        "df['Job_type'] = df['text_content'].apply(extract_job_type)\n",
        "\n",
        "print(\"Value counts for 'Job_type' column:\")\n",
        "print(df['Job_type'].value_counts())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for 'Job_type' column:\n",
            "Job_type\n",
            "None         905\n",
            "Intern       549\n",
            "Full-Time    106\n",
            "Contract      21\n",
            "Both           1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69acb004"
      },
      "source": [
        "## Extract Sponsorship Requirement\n",
        "\n",
        "### Subtask:\n",
        "Apply NLP to identify if sponsorship is required (True/False or 1/0) based on keywords and phrases in the text content related to visa sponsorship.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf74e9d6"
      },
      "source": [
        "#### Instructions\n",
        "1. Define a list of regular expression patterns to identify phrases indicating a sponsorship requirement (e.g., 'visa sponsorship', 'sponsorship available', 'require sponsorship', 'work authorization'). These patterns should be case-insensitive.\n",
        "2. Create a function that iterates through these patterns and checks if any match the 'text_content' for each row. The function should return `True` if any sponsorship-related phrase is found, and `False` otherwise.\n",
        "3. Apply this function to the 'text_content' column of the DataFrame `df`.\n",
        "4. Store the boolean result in a new column named 'Require_Sponsorship' in the DataFrame `df`.\n",
        "5. Print the value counts of the 'Require_Sponsorship' column to show the distribution of sponsorship requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ca1485"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, the first step is to define a list of regular expression patterns to identify various phrases indicating sponsorship requirements. This forms the basis for the NLP extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4e240f",
        "outputId": "b4f14db5-2d88-4d6e-ce2a-ab4b8087d0ae"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define regex patterns for sponsorship keywords\n",
        "sponsorship_patterns = [\n",
        "    r'visa\\s*sponsorship',\n",
        "    r'sponsorship\\s*available',\n",
        "    r'require\\s*sponsorship',\n",
        "    r'work\\s*authorization',\n",
        "    r'eligible\\s*to\\s*work\\s*in\\s*the\\s*us\\s*without\\s*sponsorship',\n",
        "    r'only\\s*us\\s*citizens\\s*or\\s*green\\s*card\\s*holders',\n",
        "    r'us\\s*citizen',\n",
        "    r'green\\s*card\\s*holder',\n",
        "    r'no\\s*sponsorship',\n",
        "    r'not\\s*sponsor',\n",
        "    r'can\\s*not\\s*sponsor'\n",
        "]\n",
        "\n",
        "print(\"Sponsorship patterns defined successfully.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sponsorship patterns defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c59fe0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the sponsorship patterns are defined, I will create a function to check if any of these patterns are present in the 'text_content' for each row, apply this function to create the 'Require_Sponsorship' column, and then print its value counts as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98fdc61",
        "outputId": "d12fb0cb-ebfb-43f0-b831-2d33350b020d"
      },
      "source": [
        "def check_sponsorship(text):\n",
        "    text = str(text).lower()  # Ensure text is string and lowercased\n",
        "\n",
        "    # First, check for phrases indicating NO sponsorship (higher priority if present)\n",
        "    no_sponsorship_patterns = [\n",
        "        r'no\\s*sponsorship',\n",
        "        r'not\\s*sponsor',\n",
        "        r'can\\s*not\\s*sponsor',\n",
        "        r'eligible\\s*to\\s*work\\s*in\\s*the\\s*us\\s*without\\s*sponsorship'\n",
        "    ]\n",
        "    for pattern in no_sponsorship_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return False # Explicitly states no sponsorship\n",
        "\n",
        "    # Then, check for phrases indicating sponsorship might be required or is available\n",
        "    # or implying a need for specific work authorization that may lead to sponsorship issues\n",
        "    sponsorship_needed_patterns = [\n",
        "        r'visa\\s*sponsorship',\n",
        "        r'sponsorship\\s*available',\n",
        "        r'require\\s*sponsorship',\n",
        "        r'work\\s*authorization',\n",
        "        r'only\\s*us\\s*citizens\\s*or\\s*green\\s*card\\s*holders',\n",
        "        r'us\\s*citizen',\n",
        "        r'green\\s*card\\s*holder'\n",
        "    ]\n",
        "    for pattern in sponsorship_needed_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return True # Indicates sponsorship is relevant\n",
        "\n",
        "    return False # Default if no strong indicators are found\n",
        "\n",
        "df['Require_Sponsorship'] = df['text_content'].apply(check_sponsorship)\n",
        "\n",
        "print(\"Value counts for 'Require_Sponsorship' column:\")\n",
        "print(df['Require_Sponsorship'].value_counts())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for 'Require_Sponsorship' column:\n",
            "Require_Sponsorship\n",
            "False    1321\n",
            "True      261\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483a46ea"
      },
      "source": [
        "# Task\n",
        "Consolidate and refine the skill pattern dictionary, incorporating all existing and new skill categories and their regex patterns into a single, comprehensive definition. Then, re-extract the skill sets from the 'raw_html' column of the DataFrame using this consolidated dictionary, summarize the extracted skills by listing the top 20 most frequent and the total unique skills, and finally, save the updated DataFrame to `augmented_internships_data.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2430cb18"
      },
      "source": [
        "## Consolidate Skill Patterns\n",
        "\n",
        "### Subtask:\n",
        "Create a single code cell that defines the complete `skill_patterns` dictionary. This dictionary will group specific skills under broader categories (e.g., 'Software Engineering (Backend & DevOps)', 'Business & Data Analysis') where they logically fit, and keep general technical skills (like Python, Java, R, Git) and soft skills as their own top-level entries or in new categories like 'Linux/Unix', 'Project Management & Agile', 'Communication Skills', 'Problem Solving', and 'Critical Thinking'. All regex patterns will be integrated into this single, comprehensive dictionary, eliminating redundancy from previous definitions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ee8257"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a single, comprehensive `skill_patterns` dictionary in a new code cell, incorporating all previously defined skills, grouped categories, and new categories as specified in the instructions. This consolidates the skill pattern definitions and ensures all regex patterns are included.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5c00930",
        "outputId": "2734567b-ddd1-41a1-bbcf-1c75363c948c"
      },
      "source": [
        "import re\n",
        "\n",
        "# Define the comprehensive skill_patterns dictionary\n",
        "skill_patterns = {\n",
        "\n",
        "    # ====================================================================\n",
        "    # üíª CORE PROGRAMMING & DATABASES\n",
        "    # ====================================================================\n",
        "\n",
        "    'Python': [r'python(?!.*(django|flask|data|ml))', r'pythons', r'\\bscripting\\s*languages', r'\\bscripting\\b'],\n",
        "    'Java/JVM': [r'java(?!script)', r'jvm', r'spring\\s*boot', r'hibernate', r'kotlin'],\n",
        "    'JavaScript/TypeScript': [r'javascript', r'js', r'typescript', r'ts\\b', r'nodejs', r'node\\.js', r'npm', r'yarn', r'strong\\s*programming\\s*skills\\s*in\\s*typescript/javascript'],\n",
        "    'C/C++': [r'c\\+\\+', r'cpp\\b', r'c\\s*plus\\s*plus', r'\\bc\\b(?!#)', r'experience\\s*with\\s*c\\+\\+'],\n",
        "    'Go/Rust': [r'go\\s*lang', r'golang', r'rust'],\n",
        "    'C#/.NET': [r'c\\#', r'c\\s*sharp', r'\\.net\\b', r'dot\\s*net'],\n",
        "\n",
        "    'SQL/Databases': [r'sql', r'mysql', r'postgresql', r'sqlite', r'database\\s*query', r'data\\s*warehousing', r'relational\\s*database', r'understanding\\s*of\\s*sql'],\n",
        "    'NoSQL/NewSQL': [r'nosql', r'no-sql', r'mongodb', r'cassandra', r'redis', r'dynamodb', r'understanding\\s*of\\s*no-sql\\s*databases'],\n",
        "\n",
        "    # ====================================================================\n",
        "    # üíª SOFTWARE ENGINEERING (FULL STACK, DEVOPS, QA)\n",
        "    # ====================================================================\n",
        "\n",
        "    # Frontend & Full Stack\n",
        "    'Software Engineering (Frontend & Full Stack)': [\n",
        "        r'frontend', r'full\\s*stack\\s*perspectives', r'full\\s*stack', r'ui/ux', r'user\\s*interface',\n",
        "        r'react', r'react\\.js', r'reactjs', r'nextjs', r'gatsby', r'angular', r'angular\\.js', r'vue(\\.js)?',\n",
        "        r'client-side', r'web\\s*applications', r'web\\s*technologies\\s*and\\s*concepts'\n",
        "    ],\n",
        "    'HTML/CSS': [r'html\\d*', r'css\\d*', r'sass', r'less', r'scss', r'webpack', r'babel', r'strong\\s*programming\\s*skills\\s*in\\s*.*css'],\n",
        "\n",
        "    # Web Technologies & Architecture\n",
        "    'API/Architecture': [r'api(s)?', r'rest\\s*api', r'graphql', r'microservices', r'architecture', r'system\\s*design', r'backend\\s*components', r'implementation\\s*of\\s*scalable\\s*and\\s*secure\\s*software\\s*solutions'],\n",
        "    'Web Technologies & Protocols': [r'web\\s*technologies\\s*and\\s*concepts', r'http/s', r'jwt', r'rest\\s*apis', r'graphql', r'design\\s*systems'],\n",
        "    'Mobile Development': [r'ios\\s*development', r'android\\s*development', r'swift', r'kotlin', r'objective\\s*c', r'flutter', r'react\\s*native', r'mobile\\s*application', r'xcode'],\n",
        "\n",
        "    # DevOps, CI/CD, & Observability\n",
        "    'DevOps & CI/CD': [r'devops', r'ci/cd', r'continuous\\s*integration', r'continuous\\s*deployment', r'jenkins', r'gitlab\\s*ci', r'github\\s*actions', r'pipeline(s)?', r'build\\s*automated\\s*ci/cd\\s*pipelines'],\n",
        "    'Infrastructure as Code (IaC)': [r'terraform', r'ansible', r'chef', r'puppet', r'iac\\b', r'cloudformation'],\n",
        "    'SRE/Observability': [r'site\\s*reliability\\s*engineer', r'sre\\b', r'monitoring', r'logging', r'observability', r'code\\s*observability', r'prometheus', r'grafana', r'splunk', r'elk\\s*stack', r'root\\s*cause\\s*analysis'],\n",
        "    'Containerization/Orchestration': [r'docker', r'kubernetes', r'k8s', r'container(s|ization)', r'helm\\s*chart', r'ecs', r'openshift'],\n",
        "    'Cloud Platforms (AWS/Azure/GCP)': [r'aws\\b', r'amazon\\s*web\\s*services', r'azure', r'microsoft\\s*azure', r'gcp\\b', r'google\\s*cloud\\s*platform', r'cloud\\s*computing', r'lambda', r'ec2', r's3'],\n",
        "\n",
        "    # Testing & Quality Assurance\n",
        "    'Automated Testing & QA': [\n",
        "        r'automated\\s*testing', r'qa\\b', r'quality\\s*assurance', r'application\\s*testing\\s*solutions', r'testing\\b', r'debugging\\b',\n",
        "        r'unit\\s*test(s)?', r'integration\\s*test(s)?', r'e2e\\s*testing', r'selenium', r'jest', r'cypress', r'code\\s*reviews',\n",
        "        r'software\\s*automation\\s*framework', r'build\\s*tools\\s*for\\s*automated\\s*testing', r'validate\\s*system\\s*safety', r'detect\\s*performance\\s*regressions'\n",
        "    ],\n",
        "\n",
        "    # ====================================================================\n",
        "    # üì± PRODUCT MANAGEMENT\n",
        "    # ====================================================================\n",
        "    'Product Strategy/Roadmap': [r'product\\s*management', r'product\\s*manager', r'pm\\b', r'product\\s*vision', r'roadmap', r'product\\s*strategy', r'go-to-market', r'market\\s*analysis', r'prioritized\\s*backlog\\s*items', r'digital\\s*portfolio'],\n",
        "    'User/Customer Focus': [r'user\\s*stories', r'prds', r'user\\s*research', r'customer\\s*journey', r'voice\\s*of\\s*the\\s*customer', r'persona(s)?', r'usability'],\n",
        "    'Metrics & Analysis (PM)': [r'kpis', r'metrics', r'ab\\s*testing', r'data-driven\\s*decisions', r'experimentation', r'funnel\\s*analysis', r'retention', r'engagement', r'data\\s*mining\\s*and\\s*analysis'],\n",
        "\n",
        "    # ====================================================================\n",
        "    # ü§ñ DATA SCIENCE, AI & MACHINE LEARNING\n",
        "    # ====================================================================\n",
        "    'ML/DL Concepts': [r'machine\\s*learning', r'ml\\b', r'deep\\s*learning', r'dl\\b', r'artificial\\s*intelligence', r'ai\\b', r'predictive\\s*modeling', r'model\\s*validation', r'model\\s*deployment', r'mle\\b', r'basic\\s*understanding\\s*of\\s*machine\\s*learning\\s*concepts\\s*and\\s*algorithms'],\n",
        "    'Statistical Modeling': [r'statistics', r'statistical\\s*analysis', r'hypothesis\\s*testing', r'regression', r'bayesian'],\n",
        "    'Specialized AI Fields': [r'natural\\s*language\\s*processing', r'nlp\\b', r'computer\\s*vision', r'cv\\b', r'reinforcement\\s*learning', r'generative\\s*ai', r'large\\s*language\\s*model', r'llm\\b'],\n",
        "    'Data Science Tools': [r'tensorflow', r'pytorch', r'scikit-learn', r'keras', r'hugging\\s*face', r'pandas', r'numpy', r'scipy', r'jupyter', r'rstudio'],\n",
        "    'Big Data & Processing': [r'spark', r'hadoop', r'hive', r'presto', r'kafka', r'etl\\b', r'data\\s*pipeline', r'data\\s*lake', r'databricks'],\n",
        "    'Data Analysis/Viz': [r'data\\s*science', r'data\\s*analysis', r'data\\s*analytics', r'tableau', r'power\\s*bi', r'data\\s*visualization', r'looker', r'deriving\\s*insights\\s*from\\s*large\\s*datasets'],\n",
        "\n",
        "    # ====================================================================\n",
        "    # üìà QUANTITATIVE FINANCE & FINTECH\n",
        "    # ====================================================================\n",
        "    'Quantitative Finance Core': [r'quantitative\\s*finance', r'quant\\s*analyst', r'financial\\s*modeling', r'algorithmic\\s*trading', r'high\\s*frequency\\s*trading', r'hft\\b'],\n",
        "    'Financial Instruments': [r'derivatives', r'options', r'futures', r'fixed\\s*income', r'equities', r'fx\\b', r'commodities', r'securities'],\n",
        "    'Math/Statistics (Quant)': [r'time\\s*series\\s*analysis', r'stochastic\\s*calculus', r'monte\\s*carlo\\s*simulation', r'portfolio\\s*optimization', r'black-scholes', r'calibration'],\n",
        "    'FinTech Domain': [r'domain\\s*knowledge\\s*of\\s*the\\s*futures\\s*trading\\s*industry', r'futures\\s*trading', r'trading\\s*systems', r'exchange\\s*protocols', r'fintech'],\n",
        "    'Risk/Compliance': [r'risk\\s*management', r'market\\s*risk', r'credit\\s*risk', r'regulatory\\s*compliance', r'basel\\s*\\d', r'risk\\s*culture'],\n",
        "\n",
        "    # ====================================================================\n",
        "    # üîß HARDWARE & SYSTEMS ENGINEERING\n",
        "    # ====================================================================\n",
        "    'Hardware & Systems Engineering': [r'aerospace\\s*engineering', r'electrical\\s*engineering', r'ee\\b', r'system\\s*safety', r'delivery\\s*system\\s*features', r'hardware\\s*engineering', r'circuit\\s*design', r'pcb\\s*design'],\n",
        "    'Semiconductors & IC': [r'fpga', r'asic', r'semiconductor', r'vlsi', r'ic\\s*design', r'verilog', r'vhdl', r'cadence', r'synopsys'],\n",
        "    'Systems/Firmware': [r'embedded\\s*systems', r'firmware', r'microcontroller', r'rtos', r'real-time', r'signal\\s*processing', r'dsp\\b'],\n",
        "    'Simulation & Analysis': [r'simulation(s)?', r'system\\s*level\\s*simulations', r'simulation\\s*tools', r'analyzing\\s*real\\s*world\\s*data', r'incorporating\\s*additional\\s*signals', r'analysis\\s*tools', r'execute\\s*millions\\s*of\\s*simulation\\s*scenarios'],\n",
        "\n",
        "    # ====================================================================\n",
        "    # ü§ù UNIVERSAL SKILLS & METHODOLOGIES\n",
        "    # ====================================================================\n",
        "    'Core Engineering Concepts': [r'algorithms', r'data\\s*structures', r'object-oriented\\s*programming', r'oop\\b', r'functional\\s*programming', r'design\\s*patterns', r'participate\\s*in\\s*code\\s*reviews'],\n",
        "    'Project Management & Agile': [r'agile', r'scrum', r'kanban', r'project\\s*management', r'complexity/time\\s*estimation', r'project\\s*tracking', r'sprint', r'backlog\\s*grooming'],\n",
        "    'Business/Analysis': [r'business\\s*analyst', r'data\\s*analysis', r'business\\s*intelligence', r'bi\\b', r'requirements\\s*gathering', r'process\\s*improvement', r'jira', r'confluence', r'business\\s*needs', r'business\\s*requirements'],\n",
        "    'Communication & Soft Skills': [r'communication\\s*skills', r'written\\s*communication', r'verbal\\s*communication', r'teamwork\\s*skills', r'collaboration', r'problem-solving\\s*skills', r'critical\\s*thinking', r'attention\\s*to\\s*detail', r'work\\s*independently', r'manage\\s*time\\s*effectively'],\n",
        "    'Version Control/CLI': [r'git', r'github', r'gitlab', r'bitbucket', r'version\\s*control', r'linux', r'unix', r'shell\\s*scripting', r'bash', r'command\\s*line'],\n",
        "    'Content Management': [r'contentful', r'cms\\b', r'content\\s*management\\s*system'],\n",
        "}\n",
        "\n",
        "print(\"Consolidated skill patterns defined successfully with new additions.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated skill patterns defined successfully with new additions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04b5742"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the comprehensive `skill_patterns` dictionary has been defined, I need to re-apply the `extract_skills` function to the DataFrame's 'raw_html' column to correctly populate the 'Skill_Sets' column with the newly consolidated and expanded skill definitions. I will then print the head of the 'Skill_Sets' column and the count of rows with extracted skills to verify the update.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18134c63",
        "outputId": "6b0bf976-9c4b-4677-92bb-bba4d3ebb943"
      },
      "source": [
        "def extract_skills(text):\n",
        "    text = str(text).lower()  # Convert to lowercase for case-insensitive matching\n",
        "    found_skills = set()  # Use a set to store unique skills\n",
        "\n",
        "    for skill_name, patterns in skill_patterns.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text):\n",
        "                found_skills.add(skill_name)\n",
        "                # Break from inner loop once a pattern for this skill is found\n",
        "                break\n",
        "    return sorted(list(found_skills)) # Return a sorted list of unique skills\n",
        "\n",
        "# Apply the function to the 'raw_html' column as specified in the original task (4314a9c4 used raw_html)\n",
        "df['Skill_Sets'] = df['raw_html'].apply(extract_skills)\n",
        "\n",
        "print(\"First 5 rows of re-extracted 'Skill_Sets' column with consolidated patterns:\")\n",
        "print(df['Skill_Sets'].head())\n",
        "\n",
        "print(\"\\nNumber of rows with extracted skill sets (after re-extraction with consolidated patterns):\")\n",
        "print(df[df['Skill_Sets'].apply(len) > 0].shape[0])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of re-extracted 'Skill_Sets' column with consolidated patterns:\n",
            "0    [Big Data & Processing, Business/Analysis, Clo...\n",
            "1    [API/Architecture, Automated Testing & QA, Big...\n",
            "2    [API/Architecture, Business/Analysis, C#/.NET,...\n",
            "3    [API/Architecture, Automated Testing & QA, Bus...\n",
            "4    [Business/Analysis, Communication & Soft Skill...\n",
            "Name: Skill_Sets, dtype: object\n",
            "\n",
            "Number of rows with extracted skill sets (after re-extraction with consolidated patterns):\n",
            "1524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a476f4a3"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the 'Skill_Sets' column has been re-extracted with the consolidated patterns, I will re-aggregate all extracted skills, count their frequencies, print the top 20 most frequent skills, and display the total number of unique skills found across the entire dataset to reflect these changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fabcf32",
        "outputId": "2c956137-9294-4e4f-821c-d03b4cba8a4f"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists in 'Skill_Sets' to get all individual skills\n",
        "all_skills_updated = [skill for sublist in df['Skill_Sets'] for skill in sublist]\n",
        "\n",
        "# Count the frequency of each skill\n",
        "skill_counts_updated = Counter(all_skills_updated)\n",
        "\n",
        "print(\"\\nTop 20 most frequently extracted skills (after update):\")\n",
        "for skill, count in skill_counts_updated.most_common(20):\n",
        "    print(f\"- {skill}: {count}\")\n",
        "\n",
        "print(f\"\\nTotal number of unique skills found across the dataset (after update): {len(skill_counts_updated)}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 most frequently extracted skills (after update):\n",
            "- HTML/CSS: 1524\n",
            "- JavaScript/TypeScript: 1524\n",
            "- ML/DL Concepts: 1524\n",
            "- Hardware & Systems Engineering: 1379\n",
            "- API/Architecture: 1187\n",
            "- Cloud Platforms (AWS/Azure/GCP): 1004\n",
            "- C/C++: 934\n",
            "- Version Control/CLI: 880\n",
            "- Communication & Soft Skills: 834\n",
            "- User/Customer Focus: 832\n",
            "- Automated Testing & QA: 735\n",
            "- Specialized AI Fields: 716\n",
            "- Software Engineering (Frontend & Full Stack): 655\n",
            "- Go/Rust: 640\n",
            "- Financial Instruments: 623\n",
            "- C#/.NET: 583\n",
            "- Business/Analysis: 575\n",
            "- Semiconductors & IC: 528\n",
            "- Data Analysis/Viz: 474\n",
            "- Core Engineering Concepts: 397\n",
            "\n",
            "Total number of unique skills found across the dataset (after update): 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddda106"
      },
      "source": [
        "## Save Augmented Data (Final)\n",
        "\n",
        "### Subtask:\n",
        "Save the DataFrame, now with skill sets extracted using the consolidated patterns, to the `augmented_internships_data.csv` file, ensuring all changes are permanently stored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690f4497"
      },
      "source": [
        "**Reasoning**:\n",
        "I will save the DataFrame `df` to a CSV file named `augmented_internships_data.csv`, excluding the index, to permanently store all extracted and augmented data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b3987f9",
        "outputId": "e145d298-b73d-49e6-86fc-a5aa865ae728"
      },
      "source": [
        "output_file_path = 'augmented_internships_data.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"Augmented DataFrame successfully saved to: {output_file_path}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented DataFrame successfully saved to: augmented_internships_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46abb362",
        "outputId": "5480e003-cd92-4cdc-9be2-7db4032aa1ad"
      },
      "source": [
        "selected_columns = [\n",
        "    'Company',\n",
        "    'Role',\n",
        "    'Location',\n",
        "    'Application',\n",
        "    'YOE',\n",
        "    'Diploma',\n",
        "    'Job_type',\n",
        "    'Require_Sponsorship',\n",
        "    'Skill_Sets'\n",
        "]\n",
        "\n",
        "filtered_df = df[selected_columns].copy()\n",
        "\n",
        "output_file_path_filtered = 'filtered_internships_data.csv'\n",
        "filtered_df.to_csv(output_file_path_filtered, index=False)\n",
        "\n",
        "print(f\"Filtered DataFrame successfully saved to: {output_file_path_filtered}\")\n",
        "print(\"First 5 rows of the filtered DataFrame:\")\n",
        "print(filtered_df.head())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame successfully saved to: filtered_internships_data.csv\n",
            "First 5 rows of the filtered DataFrame:\n",
            "0                  Company                               Role  \\\n",
            "0  The Walt Disney Company                Data Product Intern   \n",
            "1                Teamworks          Product Management Intern   \n",
            "2                  Zscaler     Product Management Intern - AI   \n",
            "3              üî•ServiceNow         Portfolio Associate Intern   \n",
            "4                Santander  Digital Product Management Intern   \n",
            "\n",
            "0         Location                                        Application  YOE  \\\n",
            "0  Celebration, FL  https://disney.wd5.myworkdayjobs.com/disneycar...  0.0   \n",
            "1    Remote in USA  https://ats.rippling.com/teamworks-careers/job...  3.0   \n",
            "2     San Jose, CA  https://job-boards.greenhouse.io/zscaler/jobs/...  0.0   \n",
            "3      Chicago, IL  https://jobs.smartrecruiters.com/ServiceNow/74...  0.0   \n",
            "4       Boston, MA  https://santander.wd3.myworkdayjobs.com/Santan...  0.0   \n",
            "\n",
            "0      Diploma Job_type  Require_Sponsorship  \\\n",
            "0         None     None                False   \n",
            "1       Master   Intern                False   \n",
            "2  High School   Intern                False   \n",
            "3  High School   Intern                False   \n",
            "4         None     None                False   \n",
            "\n",
            "0                                         Skill_Sets  \n",
            "0  [Big Data & Processing, Business/Analysis, Clo...  \n",
            "1  [API/Architecture, Automated Testing & QA, Big...  \n",
            "2  [API/Architecture, Business/Analysis, C#/.NET,...  \n",
            "3  [API/Architecture, Automated Testing & QA, Bus...  \n",
            "4  [Business/Analysis, Communication & Soft Skill...  \n"
          ]
        }
      ]
    }
  ]
}