{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY+HU2zZ4KJbKyA8lf0V9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/Team_4Musketeers_capstone_project/blob/Archana_Jobs_with_feature_extraction/Jobs_with_Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmpiNFjJl0mI",
        "outputId": "b70501eb-5808-4ea1-ed7b-f4e9690a683b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e9f90fb"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gradio\n",
        "\n",
        "from openai import OpenAI\n",
        "import PyPDF2\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9025c230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2748ed2-5a3b-4c74-c18e-d8426d0ca770"
      },
      "source": [
        "# Define Google Drive path for caching\n",
        "DRIVE_PATH = '/content/drive/MyDrive/job_matching_cache'\n",
        "\n",
        "# --- 1. SECURE API KEY HANDLING ---\n",
        "try:\n",
        "    # Tries to get key from Colab Secrets (Left sidebar > Key icon)\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in Colab Secrets.\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"OpenAI API Key loaded securely from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please click the Key icon on the left, add 'OPENAI_API_KEY', and enable notebook access.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. CONFIGURATION & HELPER FUNCTIONS ---\n",
        "# (Manual file upload removed as it is handled by the Gradio App)\n",
        "\n",
        "FIELD_CONFIG = {\n",
        "    \"graduation_date\":      {\"id\": 1, \"text\": \"Expected Graduation Date\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_major\": {\"id\": 2, \"text\": \"Current Degree AND Major (e.g. MS in CS)\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_gpa\":   {\"id\": 3, \"text\": \"GPA (For Current Degree Only)\", \"mode\": \"interactive\"},\n",
        "    \"us_citizenship\":       {\"id\": 4, \"text\": \"Are you a US Citizen? (If No, specify Visa Type & Sponsorship)\", \"mode\": \"interactive\"},\n",
        "    \"programming_languages\": {\"id\": 5, \"text\": \"Programming Languages\", \"mode\": \"extract\"},\n",
        "    \"experience_software\":   {\"id\": 6, \"text\": \"Work/Project Experience Summary\", \"mode\": \"extract\"},\n",
        "    \"tools_frameworks\":      {\"id\": 7, \"text\": \"Tools & Frameworks\", \"mode\": \"extract\"},\n",
        "    \"leadership\":            {\"id\": 8, \"text\": \"Leadership Experience\", \"mode\": \"extract\"},\n",
        "    \"job_preference\":        {\"id\": 9, \"text\": \"Looking for Full-time / Internship / Both?\", \"mode\": \"interactive\"},\n",
        "    \"impact_outcomes\":       {\"id\": 10,\"text\": \"Quantifiable Impact & Key Achievements\", \"mode\": \"extract\"}\n",
        "}\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def save_embeddings_and_index(index, prepared_job_data_to_save, drive_path):\n",
        "    print(\"\\n--- Saving embeddings and FAISS index to Google Drive ---\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    faiss_index_path = os.path.join(drive_path, 'faiss_index.bin')\n",
        "    job_data_path = os.path.join(drive_path, 'prepared_job_data.json')\n",
        "\n",
        "    try:\n",
        "        faiss.write_index(index, faiss_index_path)\n",
        "        print(f\"FAISS index saved to {faiss_index_path}\")\n",
        "\n",
        "        # Prepare prepared_job_data for JSON serialization\n",
        "        # Convert numpy arrays (job_embedding) to lists\n",
        "        serializable_job_data = []\n",
        "        for job_dict in prepared_job_data_to_save:\n",
        "            temp_job_dict = job_dict.copy()\n",
        "            if 'job_embedding' in temp_job_dict and isinstance(temp_job_dict['job_embedding'], np.ndarray):\n",
        "                temp_job_dict['job_embedding'] = temp_job_dict['job_embedding'].tolist()\n",
        "            serializable_job_data.append(temp_job_dict)\n",
        "\n",
        "        with open(job_data_path, 'w') as f:\n",
        "            json.dump(serializable_job_data, f, indent=4)\n",
        "        print(f\"Prepared job data saved to {job_data_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving cache files: {e}\")\n",
        "\n",
        "def load_embeddings_and_index(drive_path):\n",
        "    print(\"\\n--- Attempting to load embeddings and FAISS index from Google Drive ---\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    faiss_index_path = os.path.join(drive_path, 'faiss_index.bin')\n",
        "    job_data_path = os.path.join(drive_path, 'prepared_job_data.json')\n",
        "\n",
        "    loaded_index = None\n",
        "    loaded_prepared_job_data = None\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(faiss_index_path) and os.path.exists(job_data_path):\n",
        "            loaded_index = faiss.read_index(faiss_index_path)\n",
        "            print(f\"FAISS index loaded from {faiss_index_path}\")\n",
        "\n",
        "            with open(job_data_path, 'r') as f:\n",
        "                loaded_prepared_job_data = json.load(f)\n",
        "            print(f\"Prepared job data loaded from {job_data_path}\")\n",
        "\n",
        "            # Convert job_embedding back to numpy array\n",
        "            for job_dict in loaded_prepared_job_data:\n",
        "                if 'job_embedding' in job_dict and isinstance(job_dict['job_embedding'], list):\n",
        "                    job_dict['job_embedding'] = np.array(job_dict['job_embedding'], dtype='float32')\n",
        "\n",
        "            print(\"Cache loaded successfully.\")\n",
        "            return loaded_index, loaded_prepared_job_data\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Cache files not found.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Cache files (FAISS index or prepared job data) not found in Google Drive. Will generate new ones.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache files: {e}\")\n",
        "        return None, None"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded securely from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"------------------------------\")\n",
        "print(\"--- Pre-loading Job Data and Initializing FAISS Index (with Caching) ---\")\n",
        "\n",
        "# Attempt to load from cache first\n",
        "try:\n",
        "    loaded_index, loaded_prepared_job_data = load_embeddings_and_index(DRIVE_PATH)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading from Google Drive cache: {e}. Proceeding without cache.\")\n",
        "    loaded_index, loaded_prepared_job_data = None, None\n",
        "\n",
        "if loaded_index is not None and loaded_prepared_job_data is not None:\n",
        "    index = loaded_index\n",
        "    prepared_job_data = loaded_prepared_job_data\n",
        "    # Ensure job_type is added if loading from an older cache that didn't save it\n",
        "    for j in prepared_job_data:\n",
        "        if 'job_type' not in j:\n",
        "            r = j.get('role', '')\n",
        "            if re.search(r'intern', r, re.IGNORECASE): j['job_type'] = 'Intern'\n",
        "            elif re.search(r'new grad|graduate', r, re.IGNORECASE): j['job_type'] = 'New Grad'\n",
        "            else: j['job_type'] = 'Full-time'\n",
        "    print(\"Data and FAISS index successfully loaded from Google Drive cache.\")\n",
        "else:\n",
        "    print(\"Cache not found or failed to load. Proceeding with fresh data generation.\")\n",
        "    # --- 1. Fetch Job Postings and Parse ---\n",
        "    # Define the GitHub raw URL\n",
        "    github_raw_url = \"https://raw.githubusercontent.com/SimplifyJobs/Summer2026-Internships/dev/README.md\"\n",
        "\n",
        "    # Fetch the content of the README.md file\n",
        "    try:\n",
        "        response = requests.get(github_raw_url)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        readme_content = response.text\n",
        "        print(f\"Successfully fetched README content from {github_raw_url}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching README content: {e}\")\n",
        "        readme_content = \"\" # Initialize with empty string on error\n",
        "\n",
        "    # Define a regular expression pattern to find level 2 headings\n",
        "    heading_pattern = re.compile(r\"^##\\s(.+)$\", re.MULTILINE)\n",
        "\n",
        "    # Find all matching headings in the readme_content\n",
        "    section_headings = heading_pattern.findall(readme_content)\n",
        "\n",
        "    # Initialize an empty dictionary to store extracted DataFrames\n",
        "    all_sections_data = {}\n",
        "\n",
        "    # Iterate through each section heading to extract its content\n",
        "    for i, current_heading in enumerate(section_headings):\n",
        "        current_heading_full = f\"## {current_heading}\"\n",
        "        start_index = readme_content.find(current_heading_full)\n",
        "\n",
        "        if start_index == -1:\n",
        "            continue\n",
        "\n",
        "        end_index = -1\n",
        "        if i + 1 < len(section_headings):\n",
        "            next_heading_full = f\"## {section_headings[i+1]}\"\n",
        "            end_index = readme_content.find(next_heading_full, start_index + len(current_heading_full))\n",
        "\n",
        "        if end_index != -1:\n",
        "            section_content = readme_content[start_index + len(current_heading_full):end_index].strip()\n",
        "        else:\n",
        "            section_content = readme_content[start_index + len(current_heading_full):].strip()\n",
        "\n",
        "        section_dfs = [] # To store multiple tables if a section has them\n",
        "\n",
        "        soup = BeautifulSoup(section_content, 'lxml')\n",
        "        tables = soup.find_all('table')\n",
        "\n",
        "        for table in tables:\n",
        "            headers = []\n",
        "            if table.find('thead'):\n",
        "                for th in table.find('thead').find_all('th'):\n",
        "                    headers.append(th.get_text(strip=True))\n",
        "\n",
        "            data_rows = []\n",
        "            if table.find('tbody'):\n",
        "                for tr in table.find('tbody').find_all('tr'):\n",
        "                    row_values = []\n",
        "                    for idx, td in enumerate(tr.find_all('td')):\n",
        "                        if headers and idx < len(headers) and headers[idx] == 'Application':\n",
        "                            link = td.find('a')\n",
        "                            if link and 'href' in link.attrs:\n",
        "                                row_values.append(link['href'])\n",
        "                            else:\n",
        "                                row_values.append('')\n",
        "                        else:\n",
        "                            row_values.append(td.get_text(strip=True))\n",
        "\n",
        "                    if len(headers) > 0:\n",
        "                        if len(row_values) > len(headers):\n",
        "                            row_values = row_values[:len(headers)]\n",
        "                        elif len(row_values) < len(headers):\n",
        "                            row_values.extend([''] * (len(headers) - len(row_values))) # Pad if too few\n",
        "                        data_rows.append(row_values)\n",
        "\n",
        "            if headers and data_rows:\n",
        "                df = pd.DataFrame(data_rows, columns=headers)\n",
        "                section_dfs.append(df)\n",
        "\n",
        "        if section_dfs:\n",
        "            all_sections_data[current_heading] = section_dfs[0]\n",
        "        else:\n",
        "            all_sections_data[current_heading] = None\n",
        "    print(f\"Extracted data for {len(all_sections_data)} sections from GitHub README.\")\n",
        "\n",
        "    # --- 2. Prepare Job Data for Embedding ---\n",
        "    prepared_job_data = []\n",
        "\n",
        "    for section_name, df in all_sections_data.items():\n",
        "        if df is not None:\n",
        "            df_normalized = df.copy()\n",
        "            df_normalized.columns = df_normalized.columns.str.lower()\n",
        "\n",
        "            # FIXED: Renamed loop variable from 'index' to 'idx' to avoid shadowing the global FAISS index variable\n",
        "            for idx, row in df_normalized.iterrows():\n",
        "                company = row['company'] if 'company' in row and pd.notna(row['company']) and str(row['company']).strip() != '' else 'N/A'\n",
        "                role = row['role'] if 'role' in row and pd.notna(row['role']) and str(row['role']).strip() != '' else 'N/A'\n",
        "                location = row['location'] if 'location' in row and pd.notna(row['location']) and str(row['location']).strip() != '' else 'N/A'\n",
        "                application_link = row['application'] if 'application' in row and pd.notna(row['application']) and str(row['application']).strip() != '' else 'N/A'\n",
        "\n",
        "                embedding_text_parts = [\n",
        "                    f\"Company: {company}\",\n",
        "                    f\"Role: {role}\",\n",
        "                    f\"Location: {location}\",\n",
        "                    f\"Section: {section_name}\"\n",
        "                ]\n",
        "                embedding_text = \". \".join(embedding_text_parts)\n",
        "\n",
        "                job_details = row.to_dict()\n",
        "                job_details['application_link'] = application_link\n",
        "\n",
        "                if 'application' in job_details:\n",
        "                    del job_details['application']\n",
        "\n",
        "                job_details['section'] = section_name\n",
        "                job_details['description'] = 'N/A'\n",
        "                job_details['embedding_text'] = embedding_text\n",
        "\n",
        "                # Assign job_type based on role, to be saved with the job_details\n",
        "                r = job_details.get('role', '')\n",
        "                if re.search(r'intern', r, re.IGNORECASE): job_details['job_type'] = 'Intern'\n",
        "                elif re.search(r'new grad|graduate', r, re.IGNORECASE): job_details['job_type'] = 'New Grad'\n",
        "                else: job_details['job_type'] = 'Full-time'\n",
        "\n",
        "                prepared_job_data.append(job_details)\n",
        "    print(f\"Prepared {len(prepared_job_data)} job entries for embedding.\")\n",
        "\n",
        "    # --- 3. Generate Embeddings for Job Postings ---\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\" # Define the embedding model\n",
        "    job_embeddings = []\n",
        "\n",
        "    for i, job in enumerate(prepared_job_data):\n",
        "        try:\n",
        "            job_embedding_response = client.embeddings.create(\n",
        "                model=EMBEDDING_MODEL,\n",
        "                input=job['embedding_text']\n",
        "            )\n",
        "            job['job_embedding'] = job_embedding_response.data[0].embedding\n",
        "            job_embeddings.append(job['job_embedding'])\n",
        "            if i % 100 == 0: # Print progress every 100 jobs\n",
        "                print(f\"Generated embedding for job {i+1}/{len(prepared_job_data)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating embedding for job {i+1}: {e}\")\n",
        "            job['job_embedding'] = None\n",
        "    print(f\"Generated embeddings for {len(job_embeddings)} job postings.\")\n",
        "\n",
        "    # --- 4. Initialize FAISS Index and Add Job Embeddings ---\n",
        "    embedding_dimension = 1536 # OpenAI's 'text-embedding-ada-002' dimension is 1536\n",
        "    index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "    valid_job_embeddings = [job['job_embedding'] for job in prepared_job_data if 'job_embedding' in job and job['job_embedding'] is not None]\n",
        "    job_embeddings_np = np.array(valid_job_embeddings).astype('float32')\n",
        "\n",
        "    if len(job_embeddings_np) > 0:\n",
        "        index.add(job_embeddings_np)\n",
        "        print(f\"FAISS index initialized and {index.ntotal} job embeddings added.\")\n",
        "    else:\n",
        "        print(\"No valid job embeddings to add to the FAISS index.\")\n",
        "\n",
        "    # Save the newly generated data to cache\n",
        "    save_embeddings_and_index(index, prepared_job_data, DRIVE_PATH)\n",
        "\n",
        "print(\"Pre-loading complete.\")\n",
        "print(\"------------------------------\")\n",
        "\n",
        "# Verification steps:\n",
        "print(f\"Number of job entries in prepared_job_data: {len(prepared_job_data)}\")\n",
        "print(f\"Total vectors in FAISS index: {index.ntotal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGZgHNoCmc6m",
        "outputId": "b8ba4451-cec6-49d4-8012-5164665f2df2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "--- Pre-loading Job Data and Initializing FAISS Index (with Caching) ---\n",
            "\n",
            "--- Attempting to load embeddings and FAISS index from Google Drive ---\n",
            "Mounted at /content/drive\n",
            "FAISS index loaded from /content/drive/MyDrive/job_matching_cache/faiss_index.bin\n",
            "Prepared job data loaded from /content/drive/MyDrive/job_matching_cache/prepared_job_data.json\n",
            "Cache loaded successfully.\n",
            "Data and FAISS index successfully loaded from Google Drive cache.\n",
            "Pre-loading complete.\n",
            "------------------------------\n",
            "Number of job entries in prepared_job_data: 1140\n",
            "Total vectors in FAISS index: 1140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "1a924b8f",
        "outputId": "cb4fc974-7959-43da-85b6-ca9b1b33adaf"
      },
      "source": [
        "print(\"--- Launching Self-Contained Gradio App (Master Fix) ---\")\n",
        "\n",
        "# --- 1. Configuration & Setup ---\n",
        "\n",
        "# Define Configuration for Extraction\n",
        "FIELD_CONFIG = {\n",
        "    \"graduation_date\":      {\"id\": 1, \"text\": \"Expected Graduation Date\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_major\": {\"id\": 2, \"text\": \"Current Degree AND Major (e.g. MS in CS)\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_gpa\":   {\"id\": 3, \"text\": \"GPA (For Current Degree Only)\", \"mode\": \"interactive\"},\n",
        "    \"us_citizenship\":       {\"id\": 4, \"text\": \"Are you a US Citizen? (If No, specify Visa Type & Sponsorship)\", \"mode\": \"interactive\"},\n",
        "    \"programming_languages\": {\"id\": 5, \"text\": \"Programming Languages\", \"mode\": \"extract\"},\n",
        "    \"experience_software\":   {\"id\": 6, \"text\": \"Work/Project Experience Summary\", \"mode\": \"extract\"},\n",
        "    \"tools_frameworks\":      {\"id\": 7, \"text\": \"Tools & Frameworks\", \"mode\": \"extract\"},\n",
        "    \"leadership\":            {\"id\": 8, \"text\": \"Leadership Experience\", \"mode\": \"extract\"},\n",
        "    \"job_preference\":        {\"id\": 9, \"text\": \"Looking for Full-time / Internship / Both?\", \"mode\": \"interactive\"},\n",
        "    \"impact_outcomes\":       {\"id\": 10,\"text\": \"Quantifiable Impact & Key Achievements\", \"mode\": \"extract\"}\n",
        "}\n",
        "\n",
        "# Setup OpenAI Client\n",
        "try:\n",
        "    if 'client' not in globals():\n",
        "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "        if not OPENAI_API_KEY:\n",
        "             print(\"WARNING: OPENAI_API_KEY not found in secrets. App may fail.\")\n",
        "        else:\n",
        "             client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Client Init Error: {e}\")\n",
        "\n",
        "# Drive Path\n",
        "DRIVE_PATH = '/content/drive/MyDrive/job_matching_cache'\n",
        "\n",
        "# --- 2. Helper Functions (Extraction & Search) ---\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def extract_profile_from_resume(resume_pdf_path, client):\n",
        "    resume_text = extract_text_from_pdf(resume_pdf_path)\n",
        "    if not resume_text:\n",
        "        return {}, \"\"\n",
        "\n",
        "    openai_prompt_content = f\"\"\"\n",
        "    Analyze the resume text. Extract fields into JSON.\n",
        "    STRICT RULES:\n",
        "    - LEADERSHIP: List of objects (role, organization, description) or \"NULL\".\n",
        "    - PREFERENCE: \"Internship\", \"Full-time\", or \"Both\".\n",
        "    - NULL: Use \"NULL\" string if missing.\n",
        "\n",
        "    Required JSON:\n",
        "    {{\n",
        "        \"graduation_date\": \"value/NULL\", \"current_degree_major\": \"value/NULL\", \"current_degree_gpa\": \"value/NULL\",\n",
        "        \"us_citizenship\": \"value/NULL\", \"programming_languages\": \"value/NULL\", \"experience_software\": \"value/NULL\",\n",
        "        \"tools_frameworks\": \"value/NULL\", \"leadership\": [...], \"job_preference\": \"value/NULL\", \"impact_outcomes\": \"value/NULL\"\n",
        "    }}\n",
        "\n",
        "    RESUME:\n",
        "    {resume_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "            messages=[{\"role\": \"user\", \"content\": openai_prompt_content}]\n",
        "        )\n",
        "        extracted_data = json.loads(response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(f\"LLM Extraction Error: {e}\")\n",
        "        extracted_data = {k: \"NULL\" for k in FIELD_CONFIG.keys()}\n",
        "\n",
        "    final_output = {}\n",
        "    for key in FIELD_CONFIG.keys():\n",
        "        val = extracted_data.get(key, \"NULL\")\n",
        "        final_output[key] = \"Not Found in Resume\" if val in [\"NULL\", None, \"\"] else val\n",
        "\n",
        "    profile_parts = []\n",
        "    for k, v in final_output.items():\n",
        "        if v != \"Not Found in Resume\":\n",
        "            if isinstance(v, list) and k == \"leadership\":\n",
        "                 s = [f\"{i.get('role','')}\" for i in v]\n",
        "                 profile_parts.append(f\"{k}: {'; '.join(s)}\")\n",
        "            else:\n",
        "                 profile_parts.append(f\"{k}: {v}\")\n",
        "    return final_output, \". \".join(profile_parts)\n",
        "\n",
        "def hybrid_search_jobs(resume_embedding, candidate_preferences, all_jobs):\n",
        "    pref = str(candidate_preferences.get('job_preference', '')).lower()\n",
        "    grad_date_str = str(candidate_preferences.get('graduation_date', '')).lower()\n",
        "\n",
        "    allowed_types = set()\n",
        "    if 'internship' == pref:\n",
        "        allowed_types.add('Intern')\n",
        "    elif 'full-time' == pref:\n",
        "        allowed_types.update(['Full-time', 'New Grad'])\n",
        "    elif 'both' == pref:\n",
        "        allowed_types.update(['Intern', 'Full-time', 'New Grad'])\n",
        "    else: # This 'else' covers cases where pref is not explicitly internship/full-time/both, or is extracted as 'NULL' etc.\n",
        "        # Smart Defaults logic, now used only if preference is unclear/missing\n",
        "        import datetime\n",
        "        curr_year = datetime.datetime.now().year\n",
        "        years = [int(y) for y in re.findall(r'\\b\\d{4}\\b', grad_date_str)]\n",
        "        if years and max(years) > curr_year:\n",
        "             allowed_types = {'Intern', 'New Grad'}\n",
        "        else:\n",
        "             allowed_types = {'Full-time', 'New Grad'}\n",
        "\n",
        "    valid_jobs = [j for j in all_jobs if j.get('job_embedding') is not None and j.get('job_type', 'Full-time') in allowed_types]\n",
        "    if not valid_jobs: return []\n",
        "\n",
        "    resume_vec = np.array(resume_embedding, dtype='float32').flatten()\n",
        "    job_matrix = np.array([j['job_embedding'] for j in valid_jobs], dtype='float32')\n",
        "\n",
        "    sims = np.dot(job_matrix, resume_vec) / (np.linalg.norm(resume_vec) * np.linalg.norm(job_matrix, axis=1) + 1e-10)\n",
        "\n",
        "    results = [{'job_details': j, 'cosine_similarity': float(s)} for j, s in zip(valid_jobs, sims)]\n",
        "    results.sort(key=lambda x: x['cosine_similarity'], reverse=True)\n",
        "    return results\n",
        "\n",
        "def find_matches_with_criteria(verified_prefs, profile_text, client, prepared_jobs):\n",
        "    try:\n",
        "        emb = client.embeddings.create(model=\"text-embedding-ada-002\", input=profile_text).data[0].embedding\n",
        "    except: return [], 0.0\n",
        "\n",
        "    ranked = hybrid_search_jobs(emb, verified_prefs, prepared_jobs)\n",
        "    if not ranked: return [], 0.0\n",
        "\n",
        "    thresh = np.percentile([r['cosine_similarity'] for r in ranked], 95)\n",
        "    return [r for r in ranked if r['cosine_similarity'] >= thresh], thresh\n",
        "\n",
        "def augment_job_with_llm(job_match, profile_text, client):\n",
        "    job = job_match['job_details']\n",
        "    try:\n",
        "        res = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\":\"user\", \"content\": f\"Explain why candidate fits job in 2 sentences.\\nCandidate: {profile_text}\\nJob: {job.get('embedding_text','')}\"}],\n",
        "            max_tokens=100\n",
        "        )\n",
        "        job_match['llm_fit_summary'] = res.choices[0].message.content.strip()\n",
        "    except:\n",
        "        job_match['llm_fit_summary'] = \"Analysis failed.\"\n",
        "    return job_match\n",
        "\n",
        "# --- 3. Gradio Logic ---\n",
        "\n",
        "def extract_step(pdf_file):\n",
        "    if not pdf_file: return \"Both\", \"\", \"\", {}, \"\", \"\", gradio.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        # Load data if needed\n",
        "        global prepared_job_data\n",
        "        if 'prepared_job_data' not in globals() or prepared_job_data is None:\n",
        "             # Minimal Load Logic reuse\n",
        "             from google.colab import drive\n",
        "             drive.mount('/content/drive', force_remount=True)\n",
        "             with open(os.path.join(DRIVE_PATH, 'prepared_job_data.json'), 'r') as f:\n",
        "                 prepared_job_data = json.load(f)\n",
        "             for j in prepared_job_data:\n",
        "                 if 'job_embedding' in j: j['job_embedding'] = np.array(j['job_embedding'], dtype='float32')\n",
        "                 # The job_type should now be loaded from cache. This block ensures it's there if cache is old.\n",
        "                 if 'job_type' not in j: # Auto-tag\n",
        "                     r = j.get('role', '')\n",
        "                     if re.search(r'intern', r, re.IGNORECASE): j['job_type'] = 'Intern'\n",
        "                     elif re.search(r'new grad|graduate', r, re.IGNORECASE): j['job_type'] = 'New Grad'\n",
        "                     else: j['job_type'] = 'Full-time'\n",
        "\n",
        "        final_output, profile_text = extract_profile_from_resume(pdf_file.name, client)\n",
        "\n",
        "        pref = \"Both\"\n",
        "        raw_pref = final_output.get(\"job_preference\", \"\").lower()\n",
        "        if \"intern\" in raw_pref: pref = \"Internship\"\n",
        "        elif \"full\" in raw_pref: pref = \"Full-time\"\n",
        "\n",
        "        # Determine initial visibility of visa type textbox\n",
        "        initial_visa_visible = \"No (Specify Visa Type)\" in final_output.get(\"us_citizenship\", \"\")\n",
        "        initial_visa_value = final_output.get(\"us_citizenship\", \"\").replace(\"No (Specify Visa Type) - \", \"\") if initial_visa_visible else \"\"\n",
        "\n",
        "        msg = \"### âœ… Step 1 Complete.\\nVerify details below, then click **'Find Matching Jobs'**.\"\n",
        "        return (\n",
        "            gradio.update(value=pref, interactive=True),\n",
        "            gradio.update(value=final_output.get(\"current_degree_gpa\", \"\"), interactive=True),\n",
        "            gradio.update(value=final_output.get(\"us_citizenship\", \"\"), interactive=True),\n",
        "            final_output, profile_text, msg,\n",
        "            gradio.update(value=initial_visa_value, visible=initial_visa_visible)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return \"Both\", \"Error\", \"Error\", {}, \"\", f\"### âš ï¸ Error in Step 1:\\n{str(e)}\", gradio.update(visible=False)\n",
        "\n",
        "def display_paginated(matches, page, thresh, text, progress=gradio.Progress()):\n",
        "    start = page * 5\n",
        "    batch = matches[start:start+5]\n",
        "    if not batch: return (\"No more jobs.\", page, matches) if page > 0 else (\"No matches.\", 0, matches)\n",
        "\n",
        "    out = \"\"\n",
        "    if page == 0:\n",
        "        out += f\"### ðŸŽ¯ Found {len(matches)} matches (Top 5% shown).\\n---\\n\\n\"\n",
        "\n",
        "    for i, m in enumerate(batch):\n",
        "        progress(None, desc=f\"Analyzing {i+1}/5...\")\n",
        "        if 'llm_fit_summary' not in m: m = augment_job_with_llm(m, text, client)\n",
        "\n",
        "        job = m['job_details']\n",
        "        out += f\"### {start+i+1}. {job.get('company','N/A')} - {job.get('role','N/A')}\\n\"\n",
        "        out += f\"**Loc:** {job.get('location','N/A')} | **Sim:** {m['cosine_similarity']*100:.1f}%\\n\"\n",
        "        out += f\"**Fit:** {m.get('llm_fit_summary','')}\\n\"\n",
        "        out += f\"[Apply]({job.get('application_link','#')})\\n\\n---\\n\\n\"\n",
        "    return out, page+1, matches\n",
        "\n",
        "def search_step(pref, gpa, cit, visa_type, p_dict, p_text):\n",
        "    if not p_dict: return \"Upload resume first.\", [], 0, 0.0, {}, \"\"\n",
        "\n",
        "    # Update citizenship with visa type if applicable\n",
        "    if cit == \"No (Specify Visa Type)\" and visa_type:\n",
        "        full_citizenship_string = f\"{cit} - {visa_type}\"\n",
        "    else:\n",
        "        full_citizenship_string = cit\n",
        "\n",
        "    p_dict.update({\"job_preference\": pref, \"current_degree_gpa\": gpa, \"us_citizenship\": full_citizenship_string})\n",
        "\n",
        "    matches, thresh = find_matches_with_criteria(p_dict, p_text, client, prepared_job_data)\n",
        "    if not matches: return \"No matches found.\", [], 0, 0.0, p_dict, p_text\n",
        "\n",
        "    txt, pg, updated_matches = display_paginated(matches, 0, thresh, p_text)\n",
        "    return txt, updated_matches, pg, thresh, p_dict, p_text\n",
        "\n",
        "def toggle_visa_textbox(choice):\n",
        "    if choice == \"No (Specify Visa Type)\":\n",
        "        return gradio.update(visible=True, interactive=True)\n",
        "    else:\n",
        "        return gradio.update(visible=False, interactive=False)\n",
        "\n",
        "\n",
        "# --- 4. Launch App ---\n",
        "try: demo.close()\n",
        "except: pass\n",
        "\n",
        "with gradio.Blocks() as demo:\n",
        "    gradio.Markdown(\"# ðŸŽ¯ Resume Matcher (Master Fix)\")\n",
        "\n",
        "    state_dict = gradio.State({})\n",
        "    state_text = gradio.State(\"\")\n",
        "    state_jobs = gradio.State([])\n",
        "    state_page = gradio.State(0)\n",
        "    state_thresh = gradio.State(0.0)\n",
        "\n",
        "    with gradio.Row():\n",
        "        with gradio.Column():\n",
        "            f_in = gradio.File(label=\"1. Upload PDF\")\n",
        "            p_in = gradio.Radio([\"Internship\", \"Full-time\", \"Both\"], label=\"Preference\")\n",
        "            g_in = gradio.Textbox(label=\"GPA\")\n",
        "            c_in = gradio.Radio([\"Yes\", \"No (Specify Visa Type)\"], label=\"Citizenship\")\n",
        "            c_visa_in = gradio.Textbox(label=\"Visa Type\", visible=False, interactive=False)\n",
        "            btn = gradio.Button(\"2. Find Jobs\", variant=\"primary\")\n",
        "        with gradio.Column():\n",
        "            res_out = gradio.Markdown(\"Waiting for upload...\")\n",
        "            load_btn = gradio.Button(\"Load More\")\n",
        "\n",
        "    f_in.upload(extract_step, [f_in], [p_in, g_in, c_in, state_dict, state_text, res_out, c_visa_in])\n",
        "    c_in.change(toggle_visa_textbox, [c_in], [c_visa_in])\n",
        "    btn.click(search_step, [p_in, g_in, c_in, c_visa_in, state_dict, state_text], [res_out, state_jobs, state_page, state_thresh, state_dict, state_text])\n",
        "    load_btn.click(display_paginated, [state_jobs, state_page, state_thresh, state_text], [res_out, state_page, state_jobs])\n",
        "\n",
        "demo.launch(debug=False, share=True)\n",
        "print(\"Master App Launched.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Launching Self-Contained Gradio App (Master Fix) ---\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c81c696aa981acc346.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c81c696aa981acc346.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master App Launched.\n"
          ]
        }
      ]
    }
  ]
}