{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbyCgXPsI/T/2fdRRrR9pA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/Team_4Musketeers_capstone_project/blob/Archana_resume_AI_Agents_chatbot/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install pypdf if not already installed\n",
        "try:\n",
        "    from pypdf import PdfReader\n",
        "except ImportError:\n",
        "    !pip install pypdf\n",
        "    from pypdf import PdfReader\n",
        "\n",
        "\n",
        "# 1. SETUP & CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Warning: API Key not found. {e}\")\n",
        "    client = None\n",
        "\n",
        "# Defines the schema we want to extract\n",
        "TARGET_SCHEMA = {\n",
        "    \"graduation_date\": \"NULL\",\n",
        "    \"current_degree_major\": \"NULL\",\n",
        "    \"current_degree_gpa\": \"NULL\", # New field\n",
        "    \"us_citizenship\": \"NULL\",\n",
        "    \"visa_type\": \"NULL\", # Re-added field for visa type\n",
        "    \"programming_languages\": \"NULL\", # New field, replaces technical_skills\n",
        "    \"experience_software\": \"NULL\", # New field\n",
        "    \"tools_frameworks\": \"NULL\", # New field\n",
        "    \"leadership\": \"NULL\", # New field\n",
        "    \"job_preference\": \"NULL\"\n",
        "}\n",
        "\n",
        "# 2. AGENT 1: THE EXTRACTOR\n",
        "# ---------------------------------------------------------\n",
        "def agent_extractor(file_path):\n",
        "    \"\"\"\n",
        "    Reads PDF and uses LLM to extract initial JSON data.\n",
        "    \"\"\"\n",
        "    # Step A: OCR / Text Extraction\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"PDF Read Error: {e}\"}\n",
        "\n",
        "    # Step B: LLM Extraction\n",
        "    prompt = f\"\"\"\n",
        "    You are a Resume Parser Agent. Extract the following fields from the resume text.\n",
        "    Return ONLY valid JSON matching this structure exactly.\n",
        "    Use the string \"NULL\" if the information is not explicitly found.\n",
        "    For 'leadership', if found, output a list of dictionaries with 'role', 'organization', and 'description'. Otherwise, output 'NULL'.\n",
        "\n",
        "    Target Structure:\n",
        "    {json.dumps(TARGET_SCHEMA, indent=2)}\n",
        "\n",
        "    RESUME TEXT:\n",
        "    {text[:4000]}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # or gpt-3.5-turbo\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a JSON extractor.\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        extracted_data = json.loads(response.choices[0].message.content)\n",
        "        return extracted_data\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction Error: {e}\")\n",
        "        return TARGET_SCHEMA # Return empty schema on fail to prevent crash\n",
        "\n",
        "# 3. AGENT 2: THE INTERVIEWER\n",
        "# ---------------------------------------------------------\n",
        "def agent_interviewer(current_data, user_response=None, current_field=None):\n",
        "    \"\"\"\n",
        "    Analyzes data, updates with user response, and determines the next question.\n",
        "    Returns: (updated_data, next_field_to_ask, question_text)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Update data if user provided an answer\n",
        "    if user_response and current_field:\n",
        "        current_data[current_field] = user_response\n",
        "\n",
        "    # Special handling for us_citizenship = \"no\"\n",
        "    if current_field == \"us_citizenship\" and user_response and user_response.lower() in [\"no\", \"n\", \"false\"]:\n",
        "        if current_data.get(\"visa_type\") in [\"NULL\", \"\", None, \"null\"]:\n",
        "            current_data[\"visa_type\"] = \"NULL\" # Explicitly mark as missing to ensure it's asked next\n",
        "    # If user says yes to citizenship, ensure visa_type is not asked\n",
        "    elif current_field == \"us_citizenship\" and user_response and user_response.lower() in [\"yes\", \"y\", \"true\"]:\n",
        "        current_data[\"visa_type\"] = \"N/A\" # Not applicable if US Citizen\n",
        "\n",
        "    # 2. Find the next missing field\n",
        "    next_field = None\n",
        "    question = None\n",
        "\n",
        "    # Priority order for questions\n",
        "    fields_to_check = list(TARGET_SCHEMA.keys())\n",
        "\n",
        "    for field in fields_to_check:\n",
        "        val = current_data.get(field)\n",
        "        # Check if value is missing (NULL, empty, or None)\n",
        "        if val in [\"NULL\", \"\", None, \"null\"]:\n",
        "            next_field = field\n",
        "\n",
        "            # Generate a friendly question based on the field\n",
        "            human_field = field.replace(\"_\", \" \").title()\n",
        "            question = f\"I noticed your resume is missing **{human_field}**. could you please provide that?\"\n",
        "\n",
        "            # Special phrasing for specific fields\n",
        "            if field == \"us_citizenship\":\n",
        "                question = \"Are you a **US Citizen**? (Yes/No)\"\n",
        "            elif field == \"graduation_date\":\n",
        "                question = \"When is your expected **Graduation Date**?\"\n",
        "            elif field == \"current_degree_gpa\":\n",
        "                question = \"What is your **GPA** for your Current Degree Only?\"\n",
        "            elif field == \"job_preference\":\n",
        "                question = \"What is your specific **Job Preference** (e.g., Full-time / Internship / Both)?\"\n",
        "            elif field == \"visa_type\":\n",
        "                question = \"What is your **Visa Type** (e.g., H1B, F1-OPT)? Or if you don't need sponsorship, please state that.\"\n",
        "\n",
        "            break # Stop at the first missing field\n",
        "\n",
        "    return current_data, next_field, question\n",
        "\n",
        "# 4. AGENT 3: THE FINALIZER\n",
        "# ---------------------------------------------------------\n",
        "def agent_finalizer(final_data):\n",
        "    \"\"\"\n",
        "    Clean up data and generate a file for download.\n",
        "    \"\"\"\n",
        "    # Here you could add an LLM call to normalize dates or format text standardly\n",
        "\n",
        "    filename = \"candidate_profile.json\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(final_data, f, indent=4)\n",
        "\n",
        "    return filename, f\"**Interview Complete!**\\n\\nI have generated your profile.\\n\\n```json\\n{json.dumps(final_data, indent=2)}\\n```\"\n",
        "\n",
        "# 5. GRADIO ORCHESTRATOR (UI LOGIC)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def process_upload(file, history, state):\n",
        "    \"\"\"Triggered when file is uploaded\"\"\"\n",
        "    if not file:\n",
        "        return history, state, None\n",
        "\n",
        "    # Run Agent 1\n",
        "    extracted_data = agent_extractor(file)\n",
        "\n",
        "    # Run Agent 2 (Initial Check)\n",
        "    updated_data, next_field, question = agent_interviewer(extracted_data)\n",
        "\n",
        "    # Update State\n",
        "    state[\"data\"] = updated_data\n",
        "    state[\"current_field\"] = next_field\n",
        "\n",
        "    # Update Chat UI\n",
        "    history.append((None, \"Resume parsed! Checking for missing info...\"))\n",
        "    if question:\n",
        "        history.append((None, question))\n",
        "        return history, state, None # Don't clear file_upload yet if more questions\n",
        "    else:\n",
        "        # If by magic the resume was perfect\n",
        "        fname, msg = agent_finalizer(updated_data)\n",
        "        history.append((None, msg))\n",
        "        return history, state, fname # Clear file_upload if complete\n",
        "\n",
        "\n",
        "def process_chat(user_msg, history, state):\n",
        "    \"\"\"Triggered when user types a message\"\"\"\n",
        "    if not user_msg:\n",
        "        return history, state, None\n",
        "\n",
        "    # Get context from state\n",
        "    data = state.get(\"data\", TARGET_SCHEMA.copy())\n",
        "    current_field = state.get(\"current_field\")\n",
        "\n",
        "    # Add user message to chat history\n",
        "    history.append((user_msg, None))\n",
        "\n",
        "    # Run Agent 2 (Update & Get Next Question)\n",
        "    updated_data, next_field, question = agent_interviewer(data, user_msg, current_field)\n",
        "\n",
        "    # Update State\n",
        "    state[\"data\"] = updated_data\n",
        "    state[\"current_field\"] = next_field\n",
        "\n",
        "    # Check if we are done or need to ask more\n",
        "    if next_field:\n",
        "        history.append((None, question))\n",
        "        return history, state, None\n",
        "    else:\n",
        "        # Run Agent 3 (Finalize)\n",
        "        filename, final_msg = agent_finalizer(updated_data)\n",
        "        history.append((None, final_msg))\n",
        "        return history, state, filename\n",
        "\n",
        "# 6. UI CONSTRUCTION\n",
        "# ---------------------------------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    # State stores the JSON data and the current field being asked\n",
        "    state = gr.State({\"data\": {}, \"current_field\": None})\n",
        "\n",
        "    gr.Markdown(\"# ðŸ¤– Agentic Resume Screener\")\n",
        "    gr.Markdown(\"Upload a resume. The agents will extract data and interview you for missing details.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_upload = gr.File(label=\"1. Upload Resume (PDF)\", type=\"filepath\")\n",
        "            download_btn = gr.File(label=\"3. Download Profile\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(height=500, label=\"2. Interview Agent\", bubble_full_width=False)\n",
        "            msg_input = gr.Textbox(label=\"Your Answer\", placeholder=\"Type here and press enter...\")\n",
        "\n",
        "    # Event: File Upload\n",
        "    file_upload.change(\n",
        "        fn=process_upload,\n",
        "        inputs=[file_upload, chatbot, state],\n",
        "        outputs=[chatbot, state, download_btn]\n",
        "    )\n",
        "\n",
        "    # Event: User Chat\n",
        "    msg_input.submit(\n",
        "        fn=process_chat,\n",
        "        inputs=[msg_input, chatbot, state],\n",
        "        outputs=[chatbot, state, download_btn]\n",
        "    ).then(\n",
        "        lambda: \"\", outputs=msg_input # Clear box\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "TNhujS1iNq0W",
        "outputId": "03b64563-c40e-40e4-9af4-6e763f30b629"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3956488464.py:208: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-3956488464.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=500, label=\"2. Interview Agent\", bubble_full_width=False)\n",
            "/tmp/ipython-input-3956488464.py:222: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(height=500, label=\"2. Interview Agent\", bubble_full_width=False)\n",
            "/tmp/ipython-input-3956488464.py:222: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=500, label=\"2. Interview Agent\", bubble_full_width=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://04104d6b194e209a34.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://04104d6b194e209a34.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7d46290f4410 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://04104d6b194e209a34.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}